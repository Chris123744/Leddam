Args in experiment:
Namespace(batch_size=32, c_out=862, checkpoints='./checkpoints', d_model=256, data='custom', data_path='traffic.csv', dec_in=862, des='Exp', devices='0,1,2,3', dropout=0.5, enc_in=862, features='M', freq='h', gpu=0, is_training=1, itr=1, kernel_size=25, label_len=48, learning_rate=0.001, loss='mse', lradj='constant', model='Leddam', model_id='traffic_96_96', n_layers=3, num_workers=0, patience=6, pe_type='no', pred_len=96, revin=True, root_path='dataset/traffic/', seq_len=96, target='OT', task_name='long_term_forecast', train_epochs=100, use_amp=True, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : traffic_pl96_n_layers_3_d_model_256_dropout_0.5_pe_type_no_bs_32_lr_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 12089
val 1661
test 3413
Epoch: 1 cost time: 162.60793590545654
Epoch: 1, Steps: 377 | Train Loss: 0.334  vali_loss: 0.417   test_loss: 0.473 
Validation loss decreased (inf --> 0.417215).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 159.21940565109253
Epoch: 2, Steps: 377 | Train Loss: 0.276  vali_loss: 0.409   test_loss: 0.463 
Validation loss decreased (0.417215 --> 0.408645).  Saving model ...
Updating learning rate to 0.001
Epoch: 3 cost time: 158.22508549690247
Epoch: 3, Steps: 377 | Train Loss: 0.265  vali_loss: 0.401   test_loss: 0.454 
Validation loss decreased (0.408645 --> 0.401115).  Saving model ...
Updating learning rate to 0.001
Epoch: 4 cost time: 157.63208842277527
Epoch: 4, Steps: 377 | Train Loss: 0.259  vali_loss: 0.388   test_loss: 0.440 
Validation loss decreased (0.401115 --> 0.387969).  Saving model ...
Updating learning rate to 0.001
Epoch: 5 cost time: 157.05698370933533
Epoch: 5, Steps: 377 | Train Loss: 0.252  vali_loss: 0.386   test_loss: 0.436 
Validation loss decreased (0.387969 --> 0.385668).  Saving model ...
Updating learning rate to 0.001
Epoch: 6 cost time: 158.85742926597595
Epoch: 6, Steps: 377 | Train Loss: 0.251  vali_loss: 0.389   test_loss: 0.443 
EarlyStopping counter: 1 out of 6
Updating learning rate to 0.001
Epoch: 7 cost time: 157.94733357429504
Epoch: 7, Steps: 377 | Train Loss: 0.248  vali_loss: 0.379   test_loss: 0.428 
Validation loss decreased (0.385668 --> 0.378968).  Saving model ...
Updating learning rate to 0.001
Epoch: 8 cost time: 158.99530029296875
Epoch: 8, Steps: 377 | Train Loss: 0.247  vali_loss: 0.380   test_loss: 0.436 
EarlyStopping counter: 1 out of 6
Updating learning rate to 0.001
Epoch: 9 cost time: 159.07560896873474
Epoch: 9, Steps: 377 | Train Loss: 0.247  vali_loss: 0.377   test_loss: 0.441 
Validation loss decreased (0.378968 --> 0.376503).  Saving model ...
Updating learning rate to 0.001
Epoch: 10 cost time: 158.5843243598938
Epoch: 10, Steps: 377 | Train Loss: 0.243  vali_loss: 0.383   test_loss: 0.441 
EarlyStopping counter: 1 out of 6
Updating learning rate to 0.001
Epoch: 11 cost time: 157.69552659988403
Epoch: 11, Steps: 377 | Train Loss: 0.243  vali_loss: 0.376   test_loss: 0.435 
Validation loss decreased (0.376503 --> 0.375873).  Saving model ...
Updating learning rate to 0.001
Epoch: 12 cost time: 157.83207249641418
Epoch: 12, Steps: 377 | Train Loss: 0.242  vali_loss: 0.376   test_loss: 0.432 
Validation loss decreased (0.375873 --> 0.375815).  Saving model ...
Updating learning rate to 0.001
Epoch: 13 cost time: 159.03329944610596
Epoch: 13, Steps: 377 | Train Loss: 0.240  vali_loss: 0.376   test_loss: 0.431 
Validation loss decreased (0.375815 --> 0.375770).  Saving model ...
Updating learning rate to 0.001
Epoch: 14 cost time: 159.59134721755981
Epoch: 14, Steps: 377 | Train Loss: 0.239  vali_loss: 0.373   test_loss: 0.427 
Validation loss decreased (0.375770 --> 0.372613).  Saving model ...
Updating learning rate to 0.001
Epoch: 15 cost time: 159.00684213638306
Epoch: 15, Steps: 377 | Train Loss: 0.239  vali_loss: 0.375   test_loss: 0.431 
EarlyStopping counter: 1 out of 6
Updating learning rate to 0.001
Epoch: 16 cost time: 157.2158522605896
Epoch: 16, Steps: 377 | Train Loss: 0.239  vali_loss: 0.376   test_loss: 0.428 
EarlyStopping counter: 2 out of 6
Updating learning rate to 0.001
Epoch: 17 cost time: 157.92692756652832
Epoch: 17, Steps: 377 | Train Loss: 0.236  vali_loss: 0.371   test_loss: 0.426 
Validation loss decreased (0.372613 --> 0.371375).  Saving model ...
Updating learning rate to 0.001
Epoch: 18 cost time: 159.22962474822998
Epoch: 18, Steps: 377 | Train Loss: 0.236  vali_loss: 0.377   test_loss: 0.437 
EarlyStopping counter: 1 out of 6
Updating learning rate to 0.001
Epoch: 19 cost time: 159.57024216651917
Epoch: 19, Steps: 377 | Train Loss: 0.240  vali_loss: 0.374   test_loss: 0.430 
EarlyStopping counter: 2 out of 6
Updating learning rate to 0.001
Epoch: 20 cost time: 159.61014199256897
Epoch: 20, Steps: 377 | Train Loss: 0.235  vali_loss: 0.372   test_loss: 0.430 
EarlyStopping counter: 3 out of 6
Updating learning rate to 0.001
Epoch: 21 cost time: 159.55238890647888
Epoch: 21, Steps: 377 | Train Loss: 0.235  vali_loss: 0.374   test_loss: 0.426 
EarlyStopping counter: 4 out of 6
Updating learning rate to 0.001
Epoch: 22 cost time: 159.32836985588074
Epoch: 22, Steps: 377 | Train Loss: 0.235  vali_loss: 0.376   test_loss: 0.431 
EarlyStopping counter: 5 out of 6
Updating learning rate to 0.001
Epoch: 23 cost time: 159.80540442466736
Epoch: 23, Steps: 377 | Train Loss: 0.236  vali_loss: 0.378   test_loss: 0.432 
EarlyStopping counter: 6 out of 6
Early stopping
>>>>>>>testing : traffic_pl96_n_layers_3_d_model_256_dropout_0.5_pe_type_no_bs_32_lr_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 3413
loading model
Model weights deleted.
test shape: (3413, 96, 862) (3413, 96, 862)
mse:  0.426  mae:  0.276
