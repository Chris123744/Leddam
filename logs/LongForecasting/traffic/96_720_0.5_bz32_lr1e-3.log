Args in experiment:
Namespace(batch_size=32, c_out=862, checkpoints='./checkpoints', d_model=256, data='custom', data_path='traffic.csv', dec_in=862, des='Exp', devices='0,1,2,3', dropout=0.5, enc_in=862, features='M', freq='h', gpu=0, is_training=1, itr=1, kernel_size=25, label_len=48, learning_rate=0.001, loss='mse', lradj='constant', model='Leddam', model_id='traffic_96_720', n_layers=3, num_workers=0, patience=6, pe_type='no', pred_len=720, revin=True, root_path='dataset/traffic/', seq_len=96, target='OT', task_name='long_term_forecast', train_epochs=100, use_amp=True, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : traffic_pl720_n_layers_3_d_model_256_dropout_0.5_pe_type_no_bs_32_lr_0.001>>>>>>>>>>>>>>>>>>>>>>>>>>
train 11465
val 1037
test 2789
Epoch: 1 cost time: 196.6458933353424
Epoch: 1, Steps: 358 | Train Loss: 0.356  vali_loss: 0.464   test_loss: 0.522 
Validation loss decreased (inf --> 0.464416).  Saving model ...
Updating learning rate to 0.001
Epoch: 2 cost time: 194.61481547355652
Epoch: 2, Steps: 358 | Train Loss: 0.308  vali_loss: 0.455   test_loss: 0.513 
Validation loss decreased (0.464416 --> 0.455050).  Saving model ...
Updating learning rate to 0.001
Epoch: 3 cost time: 194.42531847953796
Epoch: 3, Steps: 358 | Train Loss: 0.296  vali_loss: 0.438   test_loss: 0.503 
Validation loss decreased (0.455050 --> 0.438071).  Saving model ...
Updating learning rate to 0.001
Epoch: 4 cost time: 190.716402053833
Epoch: 4, Steps: 358 | Train Loss: 0.288  vali_loss: 0.429   test_loss: 0.496 
Validation loss decreased (0.438071 --> 0.428828).  Saving model ...
Updating learning rate to 0.001
Epoch: 5 cost time: 191.53563928604126
Epoch: 5, Steps: 358 | Train Loss: 0.284  vali_loss: 0.427   test_loss: 0.495 
Validation loss decreased (0.428828 --> 0.426550).  Saving model ...
Updating learning rate to 0.001
Epoch: 6 cost time: 187.1847231388092
Epoch: 6, Steps: 358 | Train Loss: 0.282  vali_loss: 0.424   test_loss: 0.498 
Validation loss decreased (0.426550 --> 0.423876).  Saving model ...
Updating learning rate to 0.001
Epoch: 7 cost time: 180.30228447914124
Epoch: 7, Steps: 358 | Train Loss: 0.278  vali_loss: 0.427   test_loss: 0.501 
EarlyStopping counter: 1 out of 6
Updating learning rate to 0.001
Epoch: 8 cost time: 180.1714301109314
Epoch: 8, Steps: 358 | Train Loss: 0.277  vali_loss: 0.426   test_loss: 0.499 
EarlyStopping counter: 2 out of 6
Updating learning rate to 0.001
Epoch: 9 cost time: 178.30075025558472
Epoch: 9, Steps: 358 | Train Loss: 0.276  vali_loss: 0.427   test_loss: 0.511 
EarlyStopping counter: 3 out of 6
Updating learning rate to 0.001
Epoch: 10 cost time: 179.52700543403625
Epoch: 10, Steps: 358 | Train Loss: 0.275  vali_loss: 0.427   test_loss: 0.515 
EarlyStopping counter: 4 out of 6
Updating learning rate to 0.001
Epoch: 11 cost time: 178.87249159812927
Epoch: 11, Steps: 358 | Train Loss: 0.275  vali_loss: 0.425   test_loss: 0.508 
EarlyStopping counter: 5 out of 6
Updating learning rate to 0.001
Epoch: 12 cost time: 179.41048097610474
Epoch: 12, Steps: 358 | Train Loss: 0.273  vali_loss: 0.426   test_loss: 0.519 
EarlyStopping counter: 6 out of 6
Early stopping
>>>>>>>testing : traffic_pl720_n_layers_3_d_model_256_dropout_0.5_pe_type_no_bs_32_lr_0.001<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2789
loading model
Model weights deleted.
test shape: (2789, 720, 862) (2789, 720, 862)
mse:  0.498  mae:  0.313
